# PHyL v1.0
A massively parallel framework for regional floods and landslides modeling

#### Languages: Fortran, Python, and CMake

#### Operating systems supported: Linux, macOS, and Windows



```python
ğŸ“¦PHyL_v1.0
 â”£ ğŸ“‚Build
 â”£ ğŸ“‚DownscalingBasicData
 â”ƒ â”£ ğŸ“œTWI_coarse.asc
 â”ƒ â”£ ğŸ“œTWI_fine.asc
 â”ƒ â”£ ğŸ“œaspect_coarse.asc
 â”ƒ â”£ ğŸ“œcurvature_coarse.asc
 â”ƒ â”— ğŸ“œcurvature_fine.asc
 â”£ ğŸ“‚HydroBasics
 â”ƒ â”£ ğŸ“œDEM.asc
 â”ƒ â”£ ğŸ“œFAC.asc
 â”ƒ â”£ ğŸ“œFDR.asc
 â”ƒ â”£ ğŸ“œMask.asc
 â”ƒ â”— ğŸ“œStream.asc
 â”£ ğŸ“‚ICS
 â”ƒ â”— ğŸ“‹InitialConditions.txt
 â”£ ğŸ“‚LandslideBasics
 â”ƒ â”£ ğŸ“œDEM_fine.asc
 â”ƒ â”£ ğŸ“œSoil.asc
 â”ƒ â”£ ğŸ“œSoil_ori.asc
 â”ƒ â”£ ğŸ“œaspect_fine.asc
 â”ƒ â”£ ğŸ“œmask_fine.asc
 â”ƒ â”— ğŸ“œslope_fine.asc
 â”£ ğŸ“‚OBS
 â”ƒ â”— ğŸ“‹Yuehe_Obs.csv
 â”£ ğŸ“‚PETs (only )
 â”ƒ â”£ ğŸ“œpet2012062700.asc
 â”ƒ â”£ ğŸ“œpet2012062701.asc
 â”ƒ â”£ ğŸ“œpet2012062702.asc
 â”ƒ â”£ ğŸ“œpet2012062703.asc
 â”ƒ â”— ğŸ“œpet2012062704.asc
 â”£ ğŸ“‚Params
 â”ƒ â”£ ğŸ“œIM.asc
 â”ƒ â”£ ğŸ“œKsat.asc
 â”ƒ â”£ ğŸ“‹Parameters_hydro.txt
 â”ƒ â”£ ğŸ“‹Parameters_land.txt
 â”ƒ â”£ ğŸ“‹Parameters_parallel.txt
 â”ƒ â”£ ğŸ“œWM.asc
 â”ƒ â”— ğŸ“œcoeM.asc
 â”£ ğŸ“‚Rains
 â”ƒ â”£ ğŸ“œrain2012062700.asc
 â”ƒ â”£ ğŸ“œrain2012062701.asc
 â”ƒ â”£ ğŸ“œrain2012062702.asc
 â”ƒ â”£ ğŸ“œrain2012062703.asc
 â”ƒ â”— ğŸ“œrain2012062704.asc
 â”£ ğŸ“‚Results
 â”£ ğŸ“‚States
 â”ƒ â”£ ğŸ“œState_2012070200_SI0.asc
 â”ƒ â”£ ğŸ“œState_2012070200_SS0.asc
 â”ƒ â”— ğŸ“œState_2012070200_W0.asc
 â”£ ğŸ“‚Visualization
 â”ƒ â”£ ğŸ“‚FS
 â”ƒ â”£ ğŸ“‚PF
 â”ƒ â”£ ğŸ“‚R
 â”ƒ â”£ ğŸ“‚SM
 â”ƒ â”£ ğŸ“‚Volume
 â”ƒ â”£ ğŸ“‚W
 â”ƒ â”£ ğŸ“‘Plot_all.py
 â”ƒ â”— ğŸ“‘VideoMaker.py
 â”£ ğŸ“‚include
 â”£ ğŸ“‚logs
 â”£ ğŸ“‚src
 â”ƒ â”£ ğŸ“‘BasicModule.f90
 â”ƒ â”£ ğŸ“‘CREST_Main_Pre.f90
 â”ƒ â”£ ğŸ“‘CREST_Simu.f90
 â”ƒ â”£ ğŸ“‘Landslide_module.f90
 â”ƒ â”£ ğŸ“‘Parallel_pre.f90
 â”ƒ â”£ ğŸ“‘ReadAndWriteMatrix.f90
 â”ƒ â”£ ğŸ“‘Runoff_Routing.f90
 â”ƒ â”£ ğŸ“‘SoilDownscale_pre.f90
 â”ƒ â”£ ğŸ“‘Stability3D.f90
 â”ƒ â”£ ğŸ“‘hdf5_utils.f90
 â”ƒ â”— ğŸ“‘main.f90
 â”£ ğŸ“‹CMakeLists.txt
 â”£ ğŸ“‹Control.Project
```

#### Example of compiling and running the model in DelftBlue: The TU Delft supercomputer

- ##### Hardware

  CPU: 2x Intel XEON E5-6248R 24C 3.0GHz

  Cores: 48

  Memory: 192 GB

  SSD: 480 GB

- ##### Load the necessary modules

  DelftBlue requires loading modules hierarchically, i.e., modules need to be loaded with the right module paths. Other HPC systems are expected to behave in a similar manner. This system is based on [lmod](https://lmod.readthedocs.io/en/latest/). The module organization is hierarchical. This means, that the modules you see depend on the ones you loaded: for example, if you don't load `openmpi` you don't see `hdf5`, etc. The following are the necessary load packages:

  ```bash
  module load 2022r2
  module load openmpi
  module load gcc/11.2.0
  module load hdf5
  module load cmake
  ```

- **Compile the model and run**

  Create a new directory to store the compiled files (e.g., build)ï¼š

  ```
  mkdir build
  ```

  Enter the new directory:

  ```
  cd build
  ```

  Compile the project:

  ```
  cmake ..
  ```

  Link and built the target:

  ```
  make
  ```

  Then you will find the executable file along with the `CMakeLists.txt`. You can simply run the model by:

  ```
  ./PHyL
  ```

  Or you can submit your job using slurm workload manager with a `sbatch` command:

  ```
  sbatch name-of-your-submission-script.sh
  ```

  Finally, harvest your simulation results in the results directory. In addition, you can visualize them using our python script.
